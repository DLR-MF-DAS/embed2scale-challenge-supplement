{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f81474bf-e1fc-4fad-8974-84da9b484276",
   "metadata": {},
   "source": [
    "# Embed2Scale challenge demo notebook\n",
    "\n",
    "In this notebook we show two ways of loading the challenge task data; first through our custom challenge dataloader, and then with a dataloader from the updated SSL4EO-S12 V1.1 dataset.\n",
    "\n",
    "We then give an example of how to create a submission file by creating embeddings through random sampling.\n",
    "\n",
    "Finally, a function for testing a submission for standard errors is provided.\n",
    "\n",
    "Note that parts of this notebook is simplified for demonstration purposes. However, the datasets and dataloaders, as well as the verification of the submission file are intended to be directly usable and true to the data and the expected submission file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d53112f-bf63-468c-b41a-ecb41407c652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from challenge_dataset import E2SChallengeDataset, collate_fn\n",
    "from ssl4eos12_dataset import SSL4EOS12Dataset, S2L1C_MEAN, S2L1C_STD, S2L2A_MEAN, S2L2A_STD, S1GRD_MEAN, S1GRD_STD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a12230-d7f4-40ea-bbd0-0f0224a5d0a5",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "490a5301-3dd8-4822-bbc8-1e36c06edb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order of modalities.\n",
    "# In this demo, modalities are ordered the same as the default order in the SSL4EOS12 dataset class.\n",
    "# Modalities are loaded in the order provided here.\n",
    "# Change the order based on your needs.\n",
    "modalities = ['s2l1c', 's2l2a', 's1']\n",
    "\n",
    "# Path to challenge data folder, i.e. the folder containing the s1, s2l1c and s2l2a subfolders.\n",
    "path_to_data = '/path/to/challenge/data/'\n",
    "\n",
    "# Path to where the submission file should be written.\n",
    "path_to_output_file = 'path/to/output/file.csv'\n",
    "\n",
    "write_result_to_file = True  # Set to True to trigger saving of the csv at the end.\n",
    "\n",
    "# Create data transformation\n",
    "# Get mean and standard deviations for the modealities in the same order as the modalities\n",
    "mean_data = S2L1C_MEAN + S2L2A_MEAN + S1GRD_MEAN\n",
    "std_data = S2L1C_STD + S2L2A_STD + S1GRD_STD\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    # Add additional transformation here\n",
    "    transforms.Normalize(mean=mean_data, std=std_data)\n",
    "])\n",
    "\n",
    "# Note that both E2SChallengeDataset and SSL4EOS12Dataset outputs torch tensors, so there is no need to a ToTensor transform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1962b9a-0d1f-40f6-bb0e-6be7a0b47a51",
   "metadata": {},
   "source": [
    "# Load data with custom dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37198dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 5537\n",
      "Modality s2l1c shape: torch.Size([1, 4, 13, 264, 264])\n",
      "Modality s2l2a shape: torch.Size([1, 4, 12, 264, 264])\n",
      "Modality s1 shape: torch.Size([1, 4, 2, 264, 264])\n"
     ]
    }
   ],
   "source": [
    "# Do not concatenate modalities\n",
    "# Dataloader output is {'data': {'s2l1c': s2l1c_data, 's2l2a': s2l2a_data, 's1': s1_data}, 'file_name': file_name}\n",
    "# The data has shapes [n_samples, n_seasons, n_channels, height, width] (for s2l2a [1, 4, 12, 264, 264])\n",
    "\n",
    "concatenate_modalities = False\n",
    "dataset_e2s = E2SChallengeDataset(path_to_data, \n",
    "                               modalities = modalities, \n",
    "                               dataset_name='bands', \n",
    "                               transform=data_transform, \n",
    "                               concat=concatenate_modalities,\n",
    "                               output_file_name=False\n",
    "                              )\n",
    "\n",
    "# Print dataset length\n",
    "print(f\"Length of dataset: {len(dataset_e2s)}\")\n",
    "\n",
    "# Print shape of first sample\n",
    "for m, d in dataset_e2s[0].items():\n",
    "    print(f'Modality {m} shape:', d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc36f82a-01d2-4646-8e57-5af449a99b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 5537\n",
      "torch.Size([1, 4, 27, 264, 264])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate modalities\n",
    "# dataloader output is {'data': concatenated_data, 'file_name': file_name}\n",
    "# The data has shapes [n_samples, n_seasons, n_channels, height, width] (for concatenated_data [1, 4, 27, 264, 264])\n",
    "\n",
    "concatenate_modalities = True\n",
    "dataset_e2s = E2SChallengeDataset(path_to_data, \n",
    "                                modalities = modalities, \n",
    "                                dataset_name='bands', \n",
    "                                transform=data_transform, \n",
    "                                concat=concatenate_modalities,\n",
    "                                output_file_name=True\n",
    "                                ) \n",
    "\n",
    "# Print dataset length\n",
    "print(f\"Length of dataset: {len(dataset_e2s)}\")\n",
    "\n",
    "# Print shape of first sample\n",
    "print(dataset_e2s[0]['data'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d195113-f9f8-48cf-87af-6e0008e199e6",
   "metadata": {},
   "source": [
    "# Load with SSL4EOS12 V1.1 dataloader\n",
    "\n",
    "Note that we have modified the code to allow for a different number of samples per zarr files. The challenge task data consists of a single sample per file, while SSL4EO-S12 V1.1 has 64 samples per zarr file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c4f7d65-2f2b-45a6-a093-2ea23f87805d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 5537\n",
      "torch.Size([1, 4, 27, 264, 264])\n"
     ]
    }
   ],
   "source": [
    "dataset_ssl4eo = SSL4EOS12Dataset(\n",
    "    data_dir=path_to_data,\n",
    "    modalities=modalities, # optional, list of modality folders.\n",
    "    transform=data_transform,  # optional, torchvision transforms. Returns tensors if not provided.\n",
    "    concat=True,  # Concatenate all modalities along the band dimension.\n",
    "    single_timestamp=False,  # Load single timestamps rather than time series.\n",
    "    num_batch_samples=1,  # optional, subsample samples in each zarr file.\n",
    "    num_timestamps=4  # optional, number of seasons to include\n",
    ")\n",
    "\n",
    "# Print dataset length\n",
    "print(f\"Length of dataset: {len(dataset_ssl4eo)}\")\n",
    "\n",
    "# Print shape of first sample\n",
    "print(dataset_ssl4eo[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d55ae2a-e93c-4ae1-bd8f-c80df56779c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two datasets' first sample is the same: True\n"
     ]
    }
   ],
   "source": [
    "# Compare the output from the datasets\n",
    "print(\"The two datasets' first sample is the same:\", np.all((dataset_e2s[0]['data'] == dataset_ssl4eo[0]).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a5e4ad-27e0-4530-81f8-4048d5919b06",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "048b74dc-1e76-4a53-9190-44ab650baad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name 0: 31bb826a45...\n",
      "File name 1: 4be6443115...\n",
      "torch.Size([2, 4, 27, 264, 264])\n"
     ]
    }
   ],
   "source": [
    "train_loader  = DataLoader(\n",
    "    dataset=dataset_e2s,\n",
    "    batch_size=2,  # Note that each each challenge task zarr file contains a single sample.\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,  # Data needs to be concatenated along sample dimension instead of being stacked,\n",
    ")\n",
    "\n",
    "for ind, data_file_name in enumerate(train_loader):\n",
    "    for find, fn in enumerate(data_file_name['file_name']):\n",
    "        print(f'File name {find}:', fn[0:10] + '...')\n",
    "    print(data_file_name['data'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b02682-b177-4a59-a267-8fb4cfd0f570",
   "metadata": {},
   "source": [
    "# Create submission file\n",
    "\n",
    "In this section, we create a submission by randomly generating embeddings of the correct size.\n",
    "Finally, we create a submission file.\n",
    "\n",
    "We use the E2SChallengeDataset since we can easily get the sample ID (file name) from the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb7435c1-34cd-4dbc-8228-a25302af6cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_from_dict(emb_dict):\n",
    "    \"\"\"Assume dictionary has format {hash-id0: embedding0, hash-id1: embedding1, ...}\n",
    "    \"\"\"\n",
    "    df_submission = pd.DataFrame.from_dict(emb_dict, orient='index')\n",
    "\n",
    "    # Reset index with name 'id'\n",
    "    df_submission.index.name = 'id'\n",
    "    df_submission.reset_index(drop=False, inplace=True)\n",
    "        \n",
    "    return df_submission\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6c39f8f-c2dc-494d-8af0-59a6d3a8d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly generate embeddings from normal distribution.\n",
    "\n",
    "embedding_dim = 1024\n",
    "embeddings = {}\n",
    "rng = np.random.default_rng(seed=None)\n",
    "\n",
    "# Allow stop early for demonstration purposes\n",
    "create_n_embeddings = np.inf # 10 # np.inf\n",
    "\n",
    "# Create random embeddings before the loop to speed up this example\n",
    "rand_embds = rng.normal(0, 1, size=(len(dataset_e2s), embedding_dim))\n",
    "\n",
    "for ind, data_file_name in enumerate(dataset_e2s):\n",
    "    # -------------------------\n",
    "    # Do compression magic here\n",
    "    # -------------------------\n",
    "    \n",
    "    file_name = data_file_name['file_name']\n",
    "    \n",
    "    # Insert the random embeddings\n",
    "    emb = rand_embds[ind, :]\n",
    "    embeddings[file_name] = emb\n",
    "\n",
    "    # Stop early in this example\n",
    "    if ind >= create_n_embeddings-1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4327a7b-21e4-4c38-ad1e-f50710214d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission_file = create_submission_from_dict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57cf504c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings: 5537\n"
     ]
    }
   ],
   "source": [
    "print('Number of embeddings:', len(submission_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0ec3b23-537b-4450-87b1-9b2913c68336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>1014</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fec24d0cda8793ff55e1059c7b88763fee8d58d3decf78...</td>\n",
       "      <td>-0.006489</td>\n",
       "      <td>0.925561</td>\n",
       "      <td>-0.205256</td>\n",
       "      <td>0.403202</td>\n",
       "      <td>0.539068</td>\n",
       "      <td>0.302692</td>\n",
       "      <td>0.544578</td>\n",
       "      <td>0.073934</td>\n",
       "      <td>-1.571672</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.320863</td>\n",
       "      <td>-0.946073</td>\n",
       "      <td>0.929955</td>\n",
       "      <td>-0.086056</td>\n",
       "      <td>0.207060</td>\n",
       "      <td>-2.296272</td>\n",
       "      <td>0.543870</td>\n",
       "      <td>1.623381</td>\n",
       "      <td>0.577205</td>\n",
       "      <td>0.042881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67960f4c8870a8aa52f295da0f0fea6d708c3cee2555a4...</td>\n",
       "      <td>-0.573994</td>\n",
       "      <td>1.615674</td>\n",
       "      <td>0.280768</td>\n",
       "      <td>-2.057713</td>\n",
       "      <td>-0.330985</td>\n",
       "      <td>2.795643</td>\n",
       "      <td>0.256959</td>\n",
       "      <td>1.724288</td>\n",
       "      <td>1.108520</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.707276</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>-0.580410</td>\n",
       "      <td>0.013492</td>\n",
       "      <td>1.455911</td>\n",
       "      <td>0.189114</td>\n",
       "      <td>0.639222</td>\n",
       "      <td>0.673259</td>\n",
       "      <td>0.151533</td>\n",
       "      <td>-1.848105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9688abfaebaea5dca2ec8bde771a7bf1e2bba8e661b777...</td>\n",
       "      <td>-0.224940</td>\n",
       "      <td>0.267410</td>\n",
       "      <td>0.119751</td>\n",
       "      <td>1.129929</td>\n",
       "      <td>0.523940</td>\n",
       "      <td>-0.115578</td>\n",
       "      <td>-0.281985</td>\n",
       "      <td>-0.384090</td>\n",
       "      <td>1.541275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.621671</td>\n",
       "      <td>-0.846401</td>\n",
       "      <td>0.409987</td>\n",
       "      <td>0.367025</td>\n",
       "      <td>-1.406988</td>\n",
       "      <td>-0.136495</td>\n",
       "      <td>-0.395851</td>\n",
       "      <td>0.375316</td>\n",
       "      <td>0.299588</td>\n",
       "      <td>-0.457385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fa3ae237ee6e2ee569c20a1e088112cf2105300d9272cc...</td>\n",
       "      <td>0.495171</td>\n",
       "      <td>1.028830</td>\n",
       "      <td>0.724903</td>\n",
       "      <td>1.863715</td>\n",
       "      <td>-1.147544</td>\n",
       "      <td>-0.227417</td>\n",
       "      <td>-0.441443</td>\n",
       "      <td>0.488720</td>\n",
       "      <td>-0.212321</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.939947</td>\n",
       "      <td>-0.412391</td>\n",
       "      <td>0.029790</td>\n",
       "      <td>-0.174632</td>\n",
       "      <td>-1.031126</td>\n",
       "      <td>-0.194826</td>\n",
       "      <td>-0.099551</td>\n",
       "      <td>-1.123242</td>\n",
       "      <td>-1.268150</td>\n",
       "      <td>1.292612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>430590d31e38c5b345a92dc7d9eb8d126c01abced0cf1a...</td>\n",
       "      <td>-0.508994</td>\n",
       "      <td>-1.416999</td>\n",
       "      <td>2.414253</td>\n",
       "      <td>-1.195625</td>\n",
       "      <td>-0.722276</td>\n",
       "      <td>-1.979099</td>\n",
       "      <td>2.155417</td>\n",
       "      <td>1.318081</td>\n",
       "      <td>-0.086528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362877</td>\n",
       "      <td>-0.201678</td>\n",
       "      <td>0.788398</td>\n",
       "      <td>-0.441496</td>\n",
       "      <td>1.020406</td>\n",
       "      <td>0.859524</td>\n",
       "      <td>0.433693</td>\n",
       "      <td>1.231648</td>\n",
       "      <td>-1.031058</td>\n",
       "      <td>-0.645724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id         0         1  \\\n",
       "0  fec24d0cda8793ff55e1059c7b88763fee8d58d3decf78... -0.006489  0.925561   \n",
       "1  67960f4c8870a8aa52f295da0f0fea6d708c3cee2555a4... -0.573994  1.615674   \n",
       "2  9688abfaebaea5dca2ec8bde771a7bf1e2bba8e661b777... -0.224940  0.267410   \n",
       "3  fa3ae237ee6e2ee569c20a1e088112cf2105300d9272cc...  0.495171  1.028830   \n",
       "4  430590d31e38c5b345a92dc7d9eb8d126c01abced0cf1a... -0.508994 -1.416999   \n",
       "\n",
       "          2         3         4         5         6         7         8  ...  \\\n",
       "0 -0.205256  0.403202  0.539068  0.302692  0.544578  0.073934 -1.571672  ...   \n",
       "1  0.280768 -2.057713 -0.330985  2.795643  0.256959  1.724288  1.108520  ...   \n",
       "2  0.119751  1.129929  0.523940 -0.115578 -0.281985 -0.384090  1.541275  ...   \n",
       "3  0.724903  1.863715 -1.147544 -0.227417 -0.441443  0.488720 -0.212321  ...   \n",
       "4  2.414253 -1.195625 -0.722276 -1.979099  2.155417  1.318081 -0.086528  ...   \n",
       "\n",
       "       1014      1015      1016      1017      1018      1019      1020  \\\n",
       "0 -0.320863 -0.946073  0.929955 -0.086056  0.207060 -2.296272  0.543870   \n",
       "1 -0.707276  0.049364 -0.580410  0.013492  1.455911  0.189114  0.639222   \n",
       "2  0.621671 -0.846401  0.409987  0.367025 -1.406988 -0.136495 -0.395851   \n",
       "3 -0.939947 -0.412391  0.029790 -0.174632 -1.031126 -0.194826 -0.099551   \n",
       "4  0.362877 -0.201678  0.788398 -0.441496  1.020406  0.859524  0.433693   \n",
       "\n",
       "       1021      1022      1023  \n",
       "0  1.623381  0.577205  0.042881  \n",
       "1  0.673259  0.151533 -1.848105  \n",
       "2  0.375316  0.299588 -0.457385  \n",
       "3 -1.123242 -1.268150  1.292612  \n",
       "4  1.231648 -1.031058 -0.645724  \n",
       "\n",
       "[5 rows x 1025 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "591a16c2-d609-428a-b921-41cd32e7c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write submission\n",
    "if write_result_to_file:\n",
    "    submission_file.to_csv(path_to_output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4847ed23-ff98-46e3-a3bb-a88e7e7c6496",
   "metadata": {},
   "source": [
    "# Verify submission file integrity\n",
    "\n",
    "Below we provide a snippet from a function which will read your embeddings and test for the same errors that the evaluation will check for. The function is similar to how the submission files are loaded.\n",
    "\n",
    "The intention of this function is to help to verify that a submission has the right structure and contents, check for missing embeddings or NaN values, prior to submission.\n",
    "\n",
    "The function is intended to be a support. Successfully completing this function does not guarantee fault-free submission files, but is an indication that the most common errors are not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d5a3a24-2987-4a6b-af6d-e2888de224fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_submission(path_to_submission: str, \n",
    "                    expected_embedding_ids: set, \n",
    "                    embedding_dim: int = 1024):\n",
    "    # Load data\n",
    "    df = pd.read_csv(path_to_submission, header=0)\n",
    "\n",
    "    # Verify that id is in columns\n",
    "    if 'id' not in df.columns:\n",
    "        raise ValueError(f\"\"\"Submission file must contain column 'id'.\"\"\")\n",
    "\n",
    "    # Temporarily set index to 'id'\n",
    "    df.set_index('id', inplace=True)\n",
    "\n",
    "    # Check that all samples are included\n",
    "    submitted_embeddings = set(df.index.to_list())\n",
    "    n_missing_embeddings = len(expected_embedding_ids.difference(submitted_embeddings))\n",
    "    if n_missing_embeddings > 0:\n",
    "        raise ValueError(f\"\"\"Submission is missing {n_missing_embeddings} embeddings.\"\"\")\n",
    "    \n",
    "    # Check that embeddings have the correct length\n",
    "    if len(df.columns) != embedding_dim:\n",
    "        raise ValueError(f\"\"\"{embedding_dim} embedding dimensions, but provided embeddings have {len(df.columns)} dimensions.\"\"\")\n",
    "\n",
    "    # Convert columns to float\n",
    "    try:\n",
    "        for col in df.columns:\n",
    "            df[col] = df[col].astype(float)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"\"\"Failed to convert embedding values to float.\n",
    "    Check embeddings for any not-allowed character, for example empty strings, letters, etc.\n",
    "    Original error message: {e}\"\"\")\n",
    "\n",
    "    # Check if any NaNs \n",
    "    if df.isna().any().any():\n",
    "        raise ValueError(f\"\"\"Embeddings contain NaN values.\"\"\")\n",
    "    \n",
    "    # Successful completion of the function\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4a253e1-4c63-4210-92a3-b0ca16459f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the created embeddings as the list of all samples.\n",
    "# This can be done since we are sure to have fully looped through the dataset.\n",
    "# A better way would be to find all the IDs in the challenge data separately, e.g. from the dataloader.\n",
    "embedding_ids = set(embeddings.keys())\n",
    "\n",
    "# Test submission\n",
    "assert test_submission(path_to_output_file, embedding_ids, embedding_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
