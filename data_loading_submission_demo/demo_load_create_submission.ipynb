{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f81474bf-e1fc-4fad-8974-84da9b484276",
   "metadata": {},
   "source": [
    "# Embed2Scale challenge demo notebook\n",
    "\n",
    "In this notebook we show two ways of loading the challenge task data; first through our custom challenge dataloader, and then with a dataloader from the updated SSL4EO-S12 V1.1 dataset.\n",
    "\n",
    "We then give an example of how to create a submission file by creating embeddings through random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d53112f-bf63-468c-b41a-ecb41407c652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from challenge_dataset import E2SChallengeDataset, collate_fn\n",
    "from ssl4eos12_dataset import SSL4EOS12Dataset, S2L1C_MEAN, S2L1C_STD, S2L2A_MEAN, S2L2A_STD, S1GRD_MEAN, S1GRD_STD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a12230-d7f4-40ea-bbd0-0f0224a5d0a5",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "490a5301-3dd8-4822-bbc8-1e36c06edb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modalities = ['s2l2a', 's2l1c', 's1']\n",
    "\n",
    "# \n",
    "path_to_data = '/path/to/challenge/data/'\n",
    "path_to_output_file = 'path/to/output/file.csv'\n",
    "\n",
    "write_result_to_file = False  # Set to True to trigger saving of the csv at the end.\n",
    "\n",
    "# Create data transformation\n",
    "# Get mean and standard deviations for the modealities in the correct order\n",
    "mean_data = S2L2A_MEAN + S2L1C_MEAN + S1GRD_MEAN\n",
    "std_data = S2L2A_STD + S2L1C_STD + S1GRD_STD\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    # Add additional transformation here\n",
    "    transforms.Normalize(mean=mean_data, std=std_data)\n",
    "])\n",
    "\n",
    "# Note that both E2SChallengeDataset and SSL4EOS12Dataset outputs torch tensors, so there is no need to a ToTensor transform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1962b9a-0d1f-40f6-bb0e-6be7a0b47a51",
   "metadata": {},
   "source": [
    "# Load data with custom dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4bb85cf8-9f5e-4876-8db5-0fca4f4fdff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 5537\n",
      "Modality s2l2a shape: torch.Size([1, 4, 12, 264, 264])\n",
      "Modality s2l1c shape: torch.Size([1, 4, 13, 264, 264])\n",
      "Modality s1 shape: torch.Size([1, 4, 2, 264, 264])\n"
     ]
    }
   ],
   "source": [
    "# Do not concatenate modalities\n",
    "# Dataloader output is {'data': {'s2l2a': s2l2a_data, 's2l1c': s2l1c_data, 's1': s1_data}, 'file_name': file_name}\n",
    "# The data has shapes [n_samples, n_seasons, n_channels, height, width] (for s2l2a [1, 4, 12, 264, 264])\n",
    "\n",
    "concatenate_modalities = False\n",
    "dataset_e2s = E2SChallengeDataset(path_to_data, \n",
    "                               modalities = modalities, \n",
    "                               dataset_name='bands', \n",
    "                               transform=data_transform, \n",
    "                               concat=concatenate_modalities,\n",
    "                               output_file_name=False\n",
    "                              )\n",
    "\n",
    "# Print dataset length\n",
    "print(f\"Length of train dataset: {len(dataset_e2s)}\")\n",
    "\n",
    "# Print shape of first sample\n",
    "for m, d in dataset_e2s[0].items():\n",
    "    print(f'Modality {m} shape:', d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc36f82a-01d2-4646-8e57-5af449a99b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 5537\n",
      "torch.Size([1, 4, 27, 264, 264])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate modalities\n",
    "# dataloader output is {'data': concatenated_data, 'file_name': file_name}\n",
    "# The data has shapes [n_samples, n_seasons, n_channels, height, width] (for concatenated_data [1, 4, 27, 264, 264])\n",
    "\n",
    "concatenate_modalities = True\n",
    "dataset_e2s = E2SChallengeDataset(path_to_data, \n",
    "                                modalities = modalities, \n",
    "                                dataset_name='bands', \n",
    "                                transform=data_transform, \n",
    "                                concat=concatenate_modalities,\n",
    "                                output_file_name=True\n",
    "                                ) \n",
    "\n",
    "# Print dataset length\n",
    "print(f\"Length of train dataset: {len(dataset_e2s)}\")\n",
    "\n",
    "# Print shape of first sample\n",
    "print(dataset_e2s[0]['data'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d195113-f9f8-48cf-87af-6e0008e199e6",
   "metadata": {},
   "source": [
    "# Load with SSL4EOS12 V1.1 dataloader\n",
    "\n",
    "Note that we have modified the code to allow for a different number of samples per zarr files. The challenge task data consists of a single sample per file, while SSL4EO-S12 V1.1 has 64 samples per zarr file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c4f7d65-2f2b-45a6-a093-2ea23f87805d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 5537\n",
      "torch.Size([1, 4, 27, 264, 264])\n"
     ]
    }
   ],
   "source": [
    "dataset_ssl4eo = SSL4EOS12Dataset(\n",
    "    data_dir=path_to_data,\n",
    "    modalities=modalities, # optional, list of modality folders.\n",
    "    transform=data_transform,  # optional, torchvision transforms. Returns tensors if not provided.\n",
    "    concat=True,  # Concatenate all modalities along the band dimension.\n",
    "    single_timestamp=False,  # Load single timestamps rather than time series.\n",
    "    num_batch_samples=1,  # optional, subsample samples in each zarr file.\n",
    "    num_timestamps=4  # optional, number of seasons to include\n",
    ")\n",
    "\n",
    "# Print dataset length\n",
    "print(f\"Length of train dataset: {len(dataset_ssl4eo)}\")\n",
    "\n",
    "# Print shape of first sample\n",
    "print(dataset_ssl4eo[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d55ae2a-e93c-4ae1-bd8f-c80df56779c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two datasets' first sample is the same: True\n"
     ]
    }
   ],
   "source": [
    "# Compare the output from the datasets\n",
    "print(\"The two datasets' first sample is the same:\", np.all((dataset_e2s[0]['data'] == dataset_ssl4eo[0]).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a5e4ad-27e0-4530-81f8-4048d5919b06",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "048b74dc-1e76-4a53-9190-44ab650baad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name 0: 58c0234746...\n",
      "File name 1: 1caa53021e...\n",
      "torch.Size([2, 4, 27, 264, 264])\n"
     ]
    }
   ],
   "source": [
    "train_loader  = DataLoader(\n",
    "    dataset=dataset_e2s,\n",
    "    batch_size=2,  # Note that each each challenge task zarr file contains a single sample.\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,  # Data needs to be concatenated along sample dimension instead of being stacked,\n",
    ")\n",
    "\n",
    "for ind, data_file_name in enumerate(train_loader):\n",
    "    for find, fn in enumerate(data_file_name['file_name']):\n",
    "        print(f'File name {find}:', fn[0:10] + '...')\n",
    "    print(data_file_name['data'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b02682-b177-4a59-a267-8fb4cfd0f570",
   "metadata": {},
   "source": [
    "# Create submission file\n",
    "\n",
    "In this section, we create a submission by randomly generating embeddings of the correct size.\n",
    "Finally, we create a submission file.\n",
    "\n",
    "We use the E2SChallengeDataset since we can easily get the sample ID (file name) from the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb7435c1-34cd-4dbc-8228-a25302af6cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_format_np_array(arr):\n",
    "    \"\"\"Create string from numpy array formatted as: '[val1, val2, ...]'.\"\"\"\n",
    "    return '[' + ','.join([str(n) for n in arr]) + ']'\n",
    "\n",
    "def create_submission_from_dict(emb_dict):\n",
    "    \"\"\"Assume dictionary has format {hash-id0: embedding0, hash-id1: embedding1, ...}\n",
    "    \"\"\"\n",
    "    df_submission = pd.DataFrame(data=[[k, str_format_np_array(e)] for k, e in emb_dict.items()], \n",
    "                                 columns=['id', 'embedding'], dtype=str)\n",
    "        \n",
    "    return df_submission\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c713d0b8-ddc4-423c-88cf-02c58d1926d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly generate embeddings from normal distribution.\n",
    "\n",
    "create_n_embeddings = np.inf#10\n",
    "\n",
    "embedding_dim = 1024\n",
    "embeddings = {}\n",
    "rng = np.random.default_rng(seed=None)\n",
    "\n",
    "# Create random embeddings before the loop to speed up this example\n",
    "rand_embds = rng.normal(0, 1, size=(len(dataset_e2s), embedding_dim))\n",
    "\n",
    "for ind, data_file_name in enumerate(dataset_e2s):\n",
    "    # -------------------------\n",
    "    # Do compression magic here\n",
    "    # -------------------------\n",
    "    file_name = data_file_name['file_name']\n",
    "    # Insert the random embeddings\n",
    "    emb = rand_embds[ind, :]\n",
    "    embeddings[file_name] = emb\n",
    "\n",
    "    # Stop early in this example\n",
    "    if ind >= create_n_embeddings-1:\n",
    "        break\n",
    "\n",
    "# Create submission file\n",
    "submission_file = create_submission_from_dict(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57cf504c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embeddings: 5537\n"
     ]
    }
   ],
   "source": [
    "print('Number of embeddings:', len(submission_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e0ec3b23-537b-4450-87b1-9b2913c68336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002c11ab0bad1ae6efff695891f5713b95101063eaca5...</td>\n",
       "      <td>[-2.865617372338198,1.677732491586853,-0.60472...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002c8e787ba871d725f57833996ef31a4d60f370180a9...</td>\n",
       "      <td>[-0.3904162336796967,-0.24518438950491572,0.22...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0017ed83b4548be10f6e12e84e55b0a57ee6f67eae30ea...</td>\n",
       "      <td>[0.731157728271667,0.8078014141432319,0.175666...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0026f8260255069e3f4b29862368abe752dd7659ab2c0e...</td>\n",
       "      <td>[0.27152057155809795,-0.3736780542052399,-0.33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0031471ca1c1a720c95103d8ca90d2142abd95e27d82f6...</td>\n",
       "      <td>[0.030451769036419937,-0.7073207885596251,1.12...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  \\\n",
       "0  0002c11ab0bad1ae6efff695891f5713b95101063eaca5...   \n",
       "1  0002c8e787ba871d725f57833996ef31a4d60f370180a9...   \n",
       "2  0017ed83b4548be10f6e12e84e55b0a57ee6f67eae30ea...   \n",
       "3  0026f8260255069e3f4b29862368abe752dd7659ab2c0e...   \n",
       "4  0031471ca1c1a720c95103d8ca90d2142abd95e27d82f6...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-2.865617372338198,1.677732491586853,-0.60472...  \n",
       "1  [-0.3904162336796967,-0.24518438950491572,0.22...  \n",
       "2  [0.731157728271667,0.8078014141432319,0.175666...  \n",
       "3  [0.27152057155809795,-0.3736780542052399,-0.33...  \n",
       "4  [0.030451769036419937,-0.7073207885596251,1.12...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "591a16c2-d609-428a-b921-41cd32e7c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write submission\n",
    "if write_result_to_file:\n",
    "    submission_file.to_csv(path_to_output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
