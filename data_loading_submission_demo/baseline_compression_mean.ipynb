{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for baseline \"mean\" \n",
    "\n",
    "Example Clouds (regression/classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import zoom\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from challenge_dataset import E2SChallengeDataset, collate_fn\n",
    "from ssl4eos12_dataset import S2L1C_MEAN, S2L1C_STD, S2L2A_MEAN, S2L2A_STD, S1GRD_MEAN, S1GRD_STD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modalities = ['s2l2a', 's2l1c', 's1']\n",
    "\n",
    "mean_data = S2L2A_MEAN + S2L1C_MEAN + S1GRD_MEAN\n",
    "std_data = S2L2A_STD + S2L1C_STD + S1GRD_STD\n",
    "\n",
    "path_to_data = '/path/to/challenge/data/'\n",
    "path_to_output_file = 'path/to/output/file.csv'\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    # Add additional transformation here\n",
    "    transforms.Normalize(mean=mean_data, std=std_data)\n",
    "])\n",
    "# Note that both E2SChallengeDataset and SSL4EOS12Dataset outputs torch tensors, so there is no need to a ToTensor transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 1659\n",
      "torch.Size([1, 4, 27, 264, 264])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate modalities\n",
    "# dataloader output is {'data': concatenated_data, 'file_name': file_name}\n",
    "# The data has shapes [n_samples, n_seasons, n_channels, height, width] (for concatenated_data [1, 4, 27, 264, 264])\n",
    "\n",
    "dataset_e2s = E2SChallengeDataset(path_to_data, \n",
    "                               modalities = modalities, \n",
    "                               dataset_name='bands', \n",
    "                               transform=data_transform, \n",
    "                               concat=True,\n",
    "                               output_file_name=True\n",
    "                              )\n",
    "\n",
    "# Print dataset length\n",
    "print(f\"Length of train dataset: {len(dataset_e2s)}\")\n",
    "\n",
    "# Print shape of first sample\n",
    "print(dataset_e2s[0]['data'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name 0: 277a989511...\n",
      "File name 1: bcb433a384...\n",
      "torch.Size([2, 4, 27, 264, 264])\n"
     ]
    }
   ],
   "source": [
    "train_loader  = DataLoader(\n",
    "    dataset=dataset_e2s,\n",
    "    batch_size=1,  # Note that each each challenge task zarr file contains a single sample.\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,  # Data needs to be concatenated along sample dimension instead of being stacked,\n",
    ")\n",
    "\n",
    "for ind, data_file_name in enumerate(train_loader):\n",
    "    for find, fn in enumerate(data_file_name['file_name']):\n",
    "        print(f'File name {find}:', fn[0:10] + '...')\n",
    "    print(data_file_name['data'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create submission file\n",
    "\n",
    "In this section, we create a submission by randomly generating embeddings of the correct size.\n",
    "Finally, we create a submission file.\n",
    "\n",
    "We use the E2SChallengeDataset since we can easily get the sample ID (file name) from the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_format_np_array(arr):\n",
    "    \"\"\"Create string from numpy array formatted as: '[val1, val2, ...]'.\"\"\"\n",
    "    return '[' + ','.join([str(n) for n in arr]) + ']'\n",
    "\n",
    "def create_submission_from_dict(emb_dict):\n",
    "    \"\"\"Assume dictionary has format {hash-id0: embedding0, hash-id1: embedding1, ...}\n",
    "    \"\"\"\n",
    "    df_submission = pd.DataFrame(data=[[k, str_format_np_array(e)] for k, e in emb_dict.items()], \n",
    "                                 columns=['id', 'embedding'], dtype=str)\n",
    "        \n",
    "    return df_submission\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compress by bilinear transform and modality averaging\n",
    "\n",
    "In this section, we create a submission file by processing each sample accordingly:\n",
    "1. Subsampling each channel to 9x9 pixels using bilinear interpolation\n",
    "2. Average each modality (S1, S2 L1C, S2 L2A) in the channel dimension.\n",
    "3. Flatten into 972 element vector\n",
    "4. Append 52 zeros to the end to make the embedding 1024 element long\n",
    "\n",
    "We use the dataloader based on the E2SChallengeDataset since we can easily get the sample ID (file name) from the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress downstream task data by averaging:\n",
    "\n",
    "counter = 0\n",
    "embeddings = {}\n",
    "n_missing_numbers = 1024-4*3*9*9\n",
    "problematic_files = []\n",
    "\n",
    "for ind, data_file_name in train_loader:\n",
    "    data = data_file_name['data']\n",
    "    file_name = data_file_name['file_name']\n",
    "\n",
    "    # Embed\n",
    "    rescaled = zoom(data, (1, 1, 9/data.shape[2], 9/data.shape[3]), order=1)\n",
    "    # Mean of S2-L2A, S2-L1C and S1 respectively\n",
    "    rescaled = np.concatenate((np.mean(rescaled[:,0:12,:,:], axis=1, keepdims=True), \n",
    "                               np.mean(rescaled[:,12:25,:,:], axis=1, keepdims=True), \n",
    "                               np.mean(rescaled[:,25:,:,:], axis=1, keepdims=True)), \n",
    "                               axis=1)\n",
    "    rescaled = rescaled.flatten()\n",
    "    # append missing values\n",
    "    missing_array = np.array(n_missing_numbers*[0.])\n",
    "    embeddings[file_name] = np.concatenate((rescaled, missing_array))\n",
    "\n",
    "submission_file = create_submission_from_dict(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write submission\n",
    "if False:\n",
    "    submission_file.to_csv(path_to_output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
